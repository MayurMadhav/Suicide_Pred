{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1dbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc14f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Honetly idkI dont know what im even doing here...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Trigger warning] Excuse for self inflicted bu...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It ends tonight.I can’t do it anymore. \\nI quit.</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Everyone wants to be \"edgy\" and it's making me...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My life is over at 20 years oldHello all. I am...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "1  Am I weird I don't get affected by compliments...  non-suicide\n",
       "2  Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "3          i need helpjust help me im crying so hard      suicide\n",
       "4  I’m so lostHello, my name is Adam (16) and I’v...      suicide\n",
       "5  Honetly idkI dont know what im even doing here...      suicide\n",
       "6  [Trigger warning] Excuse for self inflicted bu...      suicide\n",
       "7   It ends tonight.I can’t do it anymore. \\nI quit.      suicide\n",
       "8  Everyone wants to be \"edgy\" and it's making me...  non-suicide\n",
       "9  My life is over at 20 years oldHello all. I am...      suicide"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing data\n",
    "data_su = pd.read_csv(\"Suicide_Detection.csv\")\n",
    "data_su.drop(\"Unnamed: 0\",inplace=True,axis=1)\n",
    "data_su.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687106c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232074, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_su.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842d7917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:50:41.231307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 18:50:41.925324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "import tensorflow as tf\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "nlp = en_core_web_sm.load()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = STOP_WORDS\n",
    "EMOJI_PATTERN = re.compile(\n",
    "    \"[\"\n",
    "    u\"U0001F600-U0001F64F\"  # emoticons\n",
    "    u\"U0001F300-U0001F5FF\"  # symbols & pictographs\n",
    "    u\"U0001F680-U0001F6FF\"  # transport & map symbols\n",
    "    u\"U0001F1E0-U0001F1FF\"  # flags (iOS)\n",
    "    u\"U00002702-U000027B0\"\n",
    "    u\"U000024C2-U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE\n",
    ")\n",
    "FILTERS = '!\"#$%&()*+,-/:;?@[\\]^_`{|}~tn'\n",
    "HTML_TAG_PATTERN = re.compile(r']*>')\n",
    "NUMBERING_PATTERN = re.compile('d+(?:st|[nr]d|th)')\n",
    "DISABLE_PIPELINES = [\"tok2vec\", \"parser\", \"ner\", \"textcat\", \"custom\", \"lemmatizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a855fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_preprocessing(text):\n",
    "    tag_removed_text = HTML_TAG_PATTERN.sub('', text)\n",
    "    emoji_removed_text = EMOJI_PATTERN.sub(r'', tag_removed_text)\n",
    "    numberings_removed_text =  NUMBERING_PATTERN.sub('', emoji_removed_text)\n",
    "\n",
    "    extra_chars_removed_text = re.sub(\n",
    "\n",
    "        r\"(.)1{2,}\",  r'11', numberings_removed_text\n",
    "\n",
    "    )\n",
    "\n",
    "    return extra_chars_removed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33761fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(doc):\n",
    "    tokens = [\n",
    "\n",
    "        token\n",
    "\n",
    "        for token in doc\n",
    "\n",
    "        if not token.is_space and\n",
    "\n",
    "           not token.like_email and\n",
    "\n",
    "           not token.like_url and\n",
    "\n",
    "           not token.is_stop and\n",
    "\n",
    "           not token.is_punct and\n",
    "\n",
    "           not token.like_num\n",
    "\n",
    "    ]\n",
    "    translation_table = str.maketrans('', '', FILTERS)\n",
    "\n",
    "    translated_tokens = [\n",
    "\n",
    "        token.text.lower().translate(translation_table)\n",
    "\n",
    "        for token in tokens\n",
    "\n",
    "    ]\n",
    "\n",
    "    lemmatized_tokens = [\n",
    "\n",
    "        lemmatizer.lemmatize(token)\n",
    "\n",
    "        for token in translated_tokens\n",
    "\n",
    "        if len(token) > 1\n",
    "\n",
    "    ]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aacb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_su['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_su['class'].iloc[:10000]\n",
    "\n",
    "labels = labels.map(lambda x: 1 if x=='non-suicide' else 0)\n",
    "\n",
    "data = data_su.iloc[:10000, :]\n",
    "\n",
    "column = 'text'\n",
    "\n",
    "not_null_data = data[data[column].notnull()]\n",
    "\n",
    "not_null_data[column] = not_null_data[column].apply(initial_preprocessing)\n",
    "\n",
    "texts = [\n",
    "    preprocess_text(doc)\n",
    "    for doc in nlp.pipe(not_null_data[column], disable=DISABLE_PIPELINES)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d591c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    filters=FILTERS,\n",
    "    lower=True\n",
    ")\n",
    "padding = 'post'\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = []\n",
    "max_sequence_len = 0\n",
    "for text in texts:\n",
    "    # convert texts to sequence\n",
    "    txt_to_seq = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequences.append(txt_to_seq)\n",
    "    # find max_sequence_len for padding\n",
    "    txt_to_seq_len = len(txt_to_seq)\n",
    "    if txt_to_seq_len > max_sequence_len:\n",
    "        max_sequence_len = txt_to_seq_len\n",
    "# post padding\n",
    "print(max_sequence_len)\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=max_sequence_len, \n",
    "    padding=padding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=max_sequence_len))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=(None, 1))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer=adam, \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b58bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    padded_sequences, \n",
    "    labels.values, \n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.plot(history.history['accuracy'])\n",
    "title = plt.title(\"History\")\n",
    "xlabel = plt.xlabel(\"Epochs\")\n",
    "ylabel = plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickled_model.predict(padded_sequences[:7])\n",
    "for pred in predictions:\n",
    "    print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e4fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_su.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2acb94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict (model,txt):\n",
    "    length = len(txt)\n",
    "    ini_txt = list(map(initial_preprocessing,txt))\n",
    "\n",
    "    texts = [\n",
    "        preprocess_text(doc)\n",
    "        for doc in nlp.pipe(ini_txt, disable=DISABLE_PIPELINES)\n",
    "    ]\n",
    "    tokenizer = Tokenizer(\n",
    "        filters=FILTERS,\n",
    "        lower=True\n",
    "    )\n",
    "    new_data_seq = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # padding\n",
    "    new_data_seq_padded = pad_sequences(new_data_seq, maxlen=1505)\n",
    "    \n",
    "    \n",
    "    predictions = model.predict(new_data_seq_padded)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95b5ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mental_state(val):\n",
    "    if val > 0.5:\n",
    "        return(\"Not-Suicide\")\n",
    "    return (\"Suicide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf18f8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "[[0.7398981]\n",
      " [0.7398981]\n",
      " [0.7398981]]\n"
     ]
    }
   ],
   "source": [
    "#custom testing\n",
    "txt = [\"what am I living for\", \"I am so happy\", \"what happede to that guy was sad\"]\n",
    "\n",
    "preds = predict(pickled_model,txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ed5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48cf76ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:50:54.193447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 18:50:54.194427: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "pickled_model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf34dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
